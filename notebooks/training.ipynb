{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30527d92",
   "metadata": {},
   "source": [
    "## ðŸ“˜ Soil Classification - PyTorch Notebook Overview\n",
    "\n",
    "### ðŸ”¹ Cell 1 â€“ Importing Libraries\n",
    "Imports all essential libraries including **PyTorch**, **torchvision**, **scikit-learn**, **PIL**, and **tqdm**. These are used for:\n",
    "- Data handling and preprocessing  \n",
    "- Model building and evaluation  \n",
    "- Image transformations and training utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5f1323",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-24T14:00:59.009596Z",
     "iopub.status.busy": "2025-05-24T14:00:59.009277Z",
     "iopub.status.idle": "2025-05-24T14:01:13.502558Z",
     "shell.execute_reply": "2025-05-24T14:01:13.501583Z"
    },
    "papermill": {
     "duration": 14.499197,
     "end_time": "2025-05-24T14:01:13.504285",
     "exception": false,
     "start_time": "2025-05-24T14:00:59.005088",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Core Python and data science libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sklearn utilities for evaluation and splitting\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Torchvision utilities for pretrained models and transforms\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "# PyTorch core libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image  # For image processing\n",
    "\n",
    "# PyTorch modules for model building and optimization\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# tqdm for progress visualization\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34990ed1",
   "metadata": {},
   "source": [
    "### ðŸ”¹ Cell 2 â€“ Device Setup and Path Configuration\n",
    "- Sets up the computation device (GPU if available, else CPU)  \n",
    "- Configures paths for:\n",
    "  - Image directories  \n",
    "  - CSV files for training and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc6b5f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T14:01:13.510752Z",
     "iopub.status.busy": "2025-05-24T14:01:13.510343Z",
     "iopub.status.idle": "2025-05-24T14:01:13.520904Z",
     "shell.execute_reply": "2025-05-24T14:01:13.519909Z"
    },
    "papermill": {
     "duration": 0.01551,
     "end_time": "2025-05-24T14:01:13.522695",
     "exception": false,
     "start_time": "2025-05-24T14:01:13.507185",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Redundant imports removed by kernel (kept for clarity during execution)\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "\n",
    "# Select GPU if available, otherwise fallback to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using:\", device)\n",
    "\n",
    "# Define base paths to the dataset folders and CSV files\n",
    "BASE_PATH = '/kaggle/input/soil-classification/soil_classification-2025'\n",
    "TRAIN_DIR = os.path.join(BASE_PATH, 'train')\n",
    "TEST_DIR = os.path.join(BASE_PATH, 'test')\n",
    "LABELS_CSV = os.path.join(BASE_PATH, 'train_labels.csv')\n",
    "TEST_IDS_CSV = os.path.join(BASE_PATH, 'test_ids.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe710cc6",
   "metadata": {},
   "source": [
    "### ðŸ”¹ Cell 3 â€“ Data Preparation and Transformations\n",
    "- Loads and encodes label data  \n",
    "- Splits the dataset into training and validation sets  \n",
    "- Applies `torchvision.transforms` for:\n",
    "  - Data augmentation (training)\n",
    "  - Normalization (both training and validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f721734",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T14:01:13.531665Z",
     "iopub.status.busy": "2025-05-24T14:01:13.531363Z",
     "iopub.status.idle": "2025-05-24T14:01:13.581547Z",
     "shell.execute_reply": "2025-05-24T14:01:13.580420Z"
    },
    "papermill": {
     "duration": 0.056842,
     "end_time": "2025-05-24T14:01:13.583573",
     "exception": false,
     "start_time": "2025-05-24T14:01:13.526731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the CSV file containing image labels\n",
    "df = pd.read_csv(LABELS_CSV)\n",
    "\n",
    "# Duplicate the image_id column to an 'image' column for convenience\n",
    "df['image'] = df['image_id']\n",
    "\n",
    "# Encode string labels (soil types) as integers\n",
    "label_mapping = {label: idx for idx, label in enumerate(df['soil_type'].unique())}\n",
    "inv_label_mapping = {v: k for k, v in label_mapping.items()}  # Inverse mapping for decoding predictions\n",
    "df['label'] = df['soil_type'].map(label_mapping)\n",
    "\n",
    "# Split dataset into training and validation sets using stratification\n",
    "train_df, val_df = train_test_split(df, test_size=0.15, stratify=df['label'], random_state=42)\n",
    "\n",
    "# Define image transformations for training, validation, and testing\n",
    "image_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "    ])\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c15d2f",
   "metadata": {},
   "source": [
    "### ðŸ”¹ Cell 4 â€“ Dataset Class and Model Setup\n",
    "- Defines a custom PyTorch `Dataset` for loading and transforming image-label pairs  \n",
    "- Creates DataLoaders for efficient batching  \n",
    "- Loads a pretrained **ResNet-18** model  \n",
    "- Modifies the final layer to match the number of soil classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7656b6bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T14:01:13.590542Z",
     "iopub.status.busy": "2025-05-24T14:01:13.589512Z",
     "iopub.status.idle": "2025-05-24T14:01:14.289298Z",
     "shell.execute_reply": "2025-05-24T14:01:14.288313Z"
    },
    "papermill": {
     "duration": 0.705082,
     "end_time": "2025-05-24T14:01:14.291247",
     "exception": false,
     "start_time": "2025-05-24T14:01:13.586165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44.7M/44.7M [00:00<00:00, 157MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Custom PyTorch dataset class for loading soil images and labels\n",
    "class SoilDataset(Dataset):\n",
    "    def __init__(self, dataframe, img_dir, transform=None, is_test=False):\n",
    "        self.df = dataframe\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.df.iloc[idx]['image']\n",
    "        img_path = os.path.join(self.img_dir, image_id)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.is_test:\n",
    "            return image, image_id  # Return image ID for test set\n",
    "        else:\n",
    "            label = self.df.iloc[idx]['label']\n",
    "            return image, label  # Return image and label for training/validation\n",
    "\n",
    "# Initialize datasets and dataloaders\n",
    "train_dataset = SoilDataset(train_df, TRAIN_DIR, transform=image_transforms['train'])\n",
    "val_dataset = SoilDataset(val_df, TRAIN_DIR, transform=image_transforms['val'])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load pretrained ResNet-18 model and modify final layer to match number of soil classes\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "weights = ResNet18_Weights.DEFAULT\n",
    "model = resnet18(weights=weights)\n",
    "model.fc = nn.Linear(model.fc.in_features, len(label_mapping))  # Replace final layer\n",
    "model = model.to(device)  # Move model to GPU/CPU\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c94d8b",
   "metadata": {},
   "source": [
    "### ðŸ”¹ Cell 5 â€“ Training Loop with F1 Evaluation\n",
    "- Implements the training loop with loss tracking  \n",
    "- Evaluates on validation data each epoch  \n",
    "- Computes:\n",
    "  - Per-class F1 scores  \n",
    "  - Minimum F1 score (used for competition evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bd638f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T14:01:14.301861Z",
     "iopub.status.busy": "2025-05-24T14:01:14.301498Z",
     "iopub.status.idle": "2025-05-24T14:27:43.021868Z",
     "shell.execute_reply": "2025-05-24T14:27:43.020615Z"
    },
    "papermill": {
     "duration": 1588.727591,
     "end_time": "2025-05-24T14:27:43.023664",
     "exception": false,
     "start_time": "2025-05-24T14:01:14.296073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [02:36<00:00,  4.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 16.5100, Min F1: 0.8788, F1s: [0.954248366013072, 0.8787878787878789, 1.0, 0.9565217391304348]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [02:27<00:00,  4.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train Loss: 4.1323, Min F1: 0.9355, F1s: [0.9681528662420382, 0.9354838709677419, 1.0, 0.9855072463768115]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [02:26<00:00,  4.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Train Loss: 3.7286, Min F1: 0.9508, F1s: [0.9746835443037974, 0.9508196721311476, 1.0, 0.9855072463768115]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [02:29<00:00,  4.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Train Loss: 2.1773, Min F1: 0.9836, F1s: [0.9873417721518988, 0.9836065573770492, 1.0, 0.9855072463768115]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [02:28<00:00,  4.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train Loss: 1.8781, Min F1: 0.9474, F1s: [0.975, 0.9473684210526316, 1.0, 0.9577464788732395]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [02:28<00:00,  4.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Train Loss: 1.7770, Min F1: 0.9855, F1s: [0.9937106918238994, 1.0, 1.0, 0.9855072463768115]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [02:28<00:00,  4.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Train Loss: 1.6376, Min F1: 0.9831, F1s: [0.9875, 0.983050847457627, 1.0, 0.9855072463768115]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [02:27<00:00,  4.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Train Loss: 1.0764, Min F1: 0.9836, F1s: [0.9873417721518988, 0.9836065573770492, 1.0, 0.9855072463768115]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [02:28<00:00,  4.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Train Loss: 0.9340, Min F1: 0.9831, F1s: [0.9937106918238994, 0.983050847457627, 1.0, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [02:27<00:00,  4.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Train Loss: 0.8765, Min F1: 0.9524, F1s: [0.9806451612903226, 0.9523809523809523, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    # Training phase\n",
    "    for images, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{EPOCHS}'):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_preds, val_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = outputs.argmax(1).cpu().numpy()\n",
    "            val_preds.extend(preds)\n",
    "            val_labels.extend(labels.numpy())\n",
    "\n",
    "    # Compute per-class F1 scores\n",
    "    f1_scores = []\n",
    "    for i in range(len(label_mapping)):\n",
    "        f1 = f1_score(np.array(val_labels) == i, np.array(val_preds) == i)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Train Loss: {train_loss:.4f}, Min F1: {min(f1_scores):.4f}, F1s: {f1_scores}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8487233a",
   "metadata": {},
   "source": [
    "### ðŸ”¹ Cell 6 â€“ Prediction and Submission File\n",
    "- Runs inference on test images  \n",
    "- Maps predicted numeric labels back to original soil types  \n",
    "- Creates `submission.csv` file in the required format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbe8188",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T14:27:43.065413Z",
     "iopub.status.busy": "2025-05-24T14:27:43.065079Z",
     "iopub.status.idle": "2025-05-24T14:28:02.673333Z",
     "shell.execute_reply": "2025-05-24T14:28:02.672081Z"
    },
    "papermill": {
     "duration": 19.631004,
     "end_time": "2025-05-24T14:28:02.675479",
     "exception": false,
     "start_time": "2025-05-24T14:27:43.044475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… submission.csv saved!\n"
     ]
    }
   ],
   "source": [
    "# Load test IDs and prepare test dataset\n",
    "test_ids = pd.read_csv(TEST_IDS_CSV)\n",
    "test_ids['image'] = test_ids['image_id']\n",
    "test_dataset = SoilDataset(test_ids, TEST_DIR, transform=image_transforms['test'], is_test=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Generate predictions\n",
    "model.eval()\n",
    "test_preds = []\n",
    "image_names = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, image_ids in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = outputs.argmax(1).cpu().numpy()\n",
    "        test_preds.extend(preds)\n",
    "        image_names.extend(image_ids)\n",
    "\n",
    "# Map numeric predictions back to original labels\n",
    "final_labels = [inv_label_mapping[p] for p in test_preds]\n",
    "submission = pd.DataFrame({\n",
    "    'image_id': image_names,\n",
    "    'soil_type': final_labels\n",
    "})\n",
    "\n",
    "# Save submission file\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"âœ… submission.csv saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c100b4",
   "metadata": {},
   "source": [
    "### ðŸ”¹ Cell 7 â€“ Notebook Completion Marker\n",
    "- Marks successful pipeline execution  \n",
    "- Outputs a final message indicating completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44250e8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T14:28:02.730579Z",
     "iopub.status.busy": "2025-05-24T14:28:02.730254Z",
     "iopub.status.idle": "2025-05-24T14:28:02.753369Z",
     "shell.execute_reply": "2025-05-24T14:28:02.752482Z"
    },
    "papermill": {
     "duration": 0.049928,
     "end_time": "2025-05-24T14:28:02.754867",
     "exception": false,
     "start_time": "2025-05-24T14:28:02.704939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>soil_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>img_cdf80d6f.jpeg</td>\n",
       "      <td>Alluvial soil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>img_c0142a80.jpg</td>\n",
       "      <td>Alluvial soil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>img_91168fb0.jpg</td>\n",
       "      <td>Alluvial soil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>img_9822190f.jpg</td>\n",
       "      <td>Alluvial soil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>img_e5fc436c.jpeg</td>\n",
       "      <td>Alluvial soil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>img_64d9cdbe.jpg</td>\n",
       "      <td>Clay soil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>img_5e5ff453.jpg</td>\n",
       "      <td>Clay soil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>img_2c4f84e3.jpg</td>\n",
       "      <td>Clay soil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>img_0a40bbe2.jpg</td>\n",
       "      <td>Clay soil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>img_3bbdb754.jpg</td>\n",
       "      <td>Clay soil</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             image_id      soil_type\n",
       "0   img_cdf80d6f.jpeg  Alluvial soil\n",
       "1    img_c0142a80.jpg  Alluvial soil\n",
       "2    img_91168fb0.jpg  Alluvial soil\n",
       "3    img_9822190f.jpg  Alluvial soil\n",
       "4   img_e5fc436c.jpeg  Alluvial soil\n",
       "..                ...            ...\n",
       "72   img_64d9cdbe.jpg      Clay soil\n",
       "73   img_5e5ff453.jpg      Clay soil\n",
       "74   img_2c4f84e3.jpg      Clay soil\n",
       "75   img_0a40bbe2.jpg      Clay soil\n",
       "76   img_3bbdb754.jpg      Clay soil\n",
       "\n",
       "[77 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load and display the submission file\n",
    "submission = pd.read_csv('submission.csv')\n",
    "submission.head(77)  # Show the first 10 predictions (you can change the number)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 12375409,
     "sourceId": 102672,
     "sourceType": "competition"
    },
    {
     "datasetId": 7495849,
     "sourceId": 11922728,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1632.039992,
   "end_time": "2025-05-24T14:28:06.301629",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-24T14:00:54.261637",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
